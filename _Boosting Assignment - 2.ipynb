{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f128d7-3ac0-48fd-921c-a13da551d72f",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de4ffe-5fc5-4253-8ca5-3406e0a57927",
   "metadata": {},
   "source": [
    "Answer- Gradient Boosting Regression is a machine learning technique used for regression tasks, which involves building an ensemble of weak regression models (typically decision trees) sequentially. Each model is trained to correct the errors made by the previous models in the ensemble, with a focus on minimizing the residual errors using gradient descent optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a7c3b-b962-4e1f-8d3c-31ca143710ef",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a1ef4-5be3-4e27-8eab-4cbe9fefca96",
   "metadata": {},
   "source": [
    "Answer- Here's a simple implementation of a gradient boosting algorithm for regression using Python and NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e1026d-420a-448e-93fb-473f0b1fb34e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGradientBoostingRegressor\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of target values\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions using the new tree\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Add the tree to the ensemble\n",
    "            self.estimators.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc2465a-db11-47b9-b232-1118ab23b974",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41620713-a44c-47b0-bc52-6a8255bb5ea1",
   "metadata": {},
   "source": [
    "Answer-To experiment with different hyperparameters such as learning rate, number of trees, and tree depth, you can use grid search or random search techniques. Here's a brief example using grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1831ed-32aa-42c9-ab53-881258bff738",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m      3\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      9\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mGradientBoostingRegressor(), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(), param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (Best Model):\", mse)\n",
    "print(\"R-squared (Best Model):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732af476-8550-4d0a-ba31-eabaccb09584",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7d49c-50d7-4ab0-a1c5-18488294c46c",
   "metadata": {},
   "source": [
    "Answer-In Gradient Boosting, a weak learner is typically a decision tree with shallow depth, often referred to as a \"stump\". These weak learners are used to sequentially fit the data and improve upon the errors of the previous models in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbabbd2-f2a7-488c-9418-c3e77ea10e5e",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2736f9a2-b4ef-4023-bc49-e3e0d5d34513",
   "metadata": {},
   "source": [
    "Answer- The intuition behind the Gradient Boosting algorithm is to iteratively fit simple models (weak learners) to the residuals of the previous models. By doing so, the algorithm focuses on reducing the errors made by the previous models, effectively improving the overall model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a9b68-c489-4a05-a780-d5750787396c",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba339847-5cf4-4b01-9673-85011d560575",
   "metadata": {},
   "source": [
    "Answer- Gradient Boosting algorithm builds an ensemble of weak learners by sequentially adding them to the ensemble. Each weak learner is trained to correct the errors made by the previous learners, with a focus on minimizing the residual errors using gradient descent optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584b22b-15ea-47db-bc35-e4db1d50f5d2",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c938c8e-6c38-4b47-b7b8-500fc7b0172e",
   "metadata": {},
   "source": [
    "Answer-Steps involved in constructing the mathematical intuition of Gradient Boosting algorithm include:\n",
    "\n",
    "1.Initialize the model with a constant value, typically the mean of the target variable.\n",
    "\n",
    "2.Fit a weak learner (e.g., decision tree) to the residuals between the predictions and the actual target values.\n",
    "    \n",
    "3.Update the model by adding the predictions from the weak learner, multiplied by a small learning rate, to the previous model predictions.\n",
    "\n",
    "4.Repeat steps 2 and 3 until a predefined number of iterations or until a stopping criterion is met, such as reaching a minimum error threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f3f4a-6730-444a-acab-3b3bdeb13546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
